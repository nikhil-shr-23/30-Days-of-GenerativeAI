{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Preprocessing Basics\n",
    "\n",
    "This notebook demonstrates fundamental NLP preprocessing techniques using NLTK and spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install nltk spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello world! This is an example of tokenization in NLP.\"\n",
    "\n",
    "# NLTK Tokenization\n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "print(\"NLTK Tokens:\", nltk_tokens)\n",
    "\n",
    "# spaCy Tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "print(\"spaCy Tokens:\", spacy_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stemming & Lemmatization\n",
    "\n",
    "Stemming and lemmatization reduce words to their base or root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download WordNet for lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\", \"computers\", \"better\", \"was\", \"is\"]\n",
    "\n",
    "# Stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "\n",
    "# spaCy lemmatization\n",
    "doc = nlp(\" \".join(words))\n",
    "spacy_lemmas = [token.lemma_ for token in doc]\n",
    "print(\"spaCy lemmas:\", spacy_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stopword Removal\n",
    "\n",
    "Stopwords are common words that often don't contribute much meaning (e.g., \"the\", \"is\", \"at\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is an example sentence showing stopword removal in action.\"\n",
    "\n",
    "# NLTK stopword removal\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "nltk_tokens = nltk.word_tokenize(sentence.lower())\n",
    "nltk_filtered = [word for word in nltk_tokens if word not in nltk_stopwords]\n",
    "print(\"Original sentence:\", sentence)\n",
    "print(\"After NLTK stopword removal:\", nltk_filtered)\n",
    "\n",
    "# spaCy stopword removal\n",
    "doc = nlp(sentence)\n",
    "spacy_filtered = [token.text for token in doc if not token.is_stop]\n",
    "print(\"After spaCy stopword removal:\", spacy_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part-of-Speech (POS) Tagging\n",
    "\n",
    "POS tagging identifies the grammatical parts of speech in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download NLTK resources for POS tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# NLTK POS tagging\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "nltk_pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"NLTK POS Tags:\", nltk_pos_tags)\n",
    "\n",
    "# spaCy POS tagging\n",
    "doc = nlp(sentence)\n",
    "spacy_pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "print(\"spaCy POS Tags:\", spacy_pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition (NER)\n",
    "\n",
    "NER identifies entities like people, organizations, locations, etc. in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample text for NER\n",
    "text = \"Apple Inc. is planning to open a new store in New York City next month. CEO Tim Cook made the announcement yesterday.\"\n",
    "\n",
    "# spaCy NER\n",
    "doc = nlp(text)\n",
    "print(\"Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"- {ent.text} ({ent.label_}): {spacy.explain(ent.label_)}\")\n",
    "\n",
    "# Visualize entities (if in Jupyter notebook)\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Challenge\n",
    "\n",
    "Take a paragraph from Wikipedia, preprocess it, and compare results between NLTK and spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample Wikipedia paragraph\n",
    "wiki_text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language, in particular how to program computers \n",
    "to process and analyze large amounts of natural language data. The goal is a computer capable of understanding \n",
    "the contents of documents, including the contextual nuances of the language within them.\n",
    "\"\"\"\n",
    "\n",
    "# Your preprocessing code here\n",
    "# 1. Tokenize\n",
    "# 2. Remove stopwords\n",
    "# 3. Lemmatize\n",
    "# 4. Compare results\n",
    "\n",
    "# NLTK processing\n",
    "nltk_tokens = nltk.word_tokenize(wiki_text)\n",
    "nltk_filtered = [word.lower() for word in nltk_tokens if word.lower() not in nltk_stopwords and word.isalnum()]\n",
    "nltk_lemmatized = [lemmatizer.lemmatize(word) for word in nltk_filtered]\n",
    "\n",
    "# spaCy processing\n",
    "doc = nlp(wiki_text)\n",
    "spacy_filtered = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "print(\"NLTK processed tokens (first 20):\", nltk_lemmatized[:20])\n",
    "print(\"spaCy processed tokens (first 20):\", spacy_filtered[:20])\n",
    "\n",
    "# Compare unique tokens\n",
    "nltk_set = set(nltk_lemmatized)\n",
    "spacy_set = set(spacy_filtered)\n",
    "\n",
    "print(f\"\\nUnique tokens in NLTK: {len(nltk_set)}\")\n",
    "print(f\"Unique tokens in spaCy: {len(spacy_set)}\")\n",
    "print(f\"Tokens in NLTK but not in spaCy: {len(nltk_set - spacy_set)}\")\n",
    "print(f\"Tokens in spaCy but not in NLTK: {len(spacy_set - nltk_set)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}